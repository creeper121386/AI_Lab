# whyの学习笔记 深度视觉识别（from CS231n）

## 1.激活函数

### sigmoid函数：$\sigma(x)=\sum_{i=1}^n(w_ix_i+b)$： 

* 使用了指数，计算量大
* 由于有饱和区域，在输入较大的$x$时，会导致梯度消失（在饱和区梯度为0）
* 当训练数据全为正或全为负时，会导致梯度更新很慢。由于其公式:

$$\sigma(x)=\sum_{i=1}^n(w_ix_i+b)$$

使得计算图中$\sigma$节点处总有$\frac{\partial \sigma}{\partial w}=x$，使得$w$的更新只能向着梯度全为正（或全为负）的方向更新，使得更新效率极低。

解决办法：将数据归一化到均值为$0$的区间内，如$[-1,1]$

### tanh函数：$tanh(x)$

* 由于有饱和区域，仍然会导致梯度消失（在饱和区梯度为0）

### ReLU函数：$f(x)=\max(0,x)$

* 收敛更快（大概是sigmoid的6倍）
* 在正半轴不会出现梯度消失
* 符合生物学理论
* 计算成本低

* 在负半轴处有饱和区，会有梯度消失（dead ReLU）
* 不是以$0$为中心

### 参数整流器(PReLU)：$f(x)=\max(\alpha x,x)$
#### Leaky ReLU：$f(x)=\max(0.01x,x)$

* 在整个实数域上没有饱和区，完全不会出现梯度消失。
* 同时也具有ReLU的优点

### ELU函数：
* 表达式为：$f(x)=\begin{cases} x \qquad if \ x>0\\\alpha(e^x-1) \qquad if \ x\leqslant0  \end{cases}$

* 具有ReLU的优点
* 对噪音有更强的鲁棒性

### maxout函数：$\max(W^Tx_1+b_1,W^Tx_2+b_2)$

## 权重初始化

* 当网络很深时，不要将权重初始化为很小的值
* Xavier初始化：每层的权重$W_{m,n}$都从标准高斯分布中随机采样，并将结果除以$\sqrt{m}$。即：
```python
W = np.random.randn(m, n) / np.sqrt(m)
``` 
* 使用Xavier初始化时，如果对应层使用ReLU函数，需要将$W$除以$2$（因为有一般的输入数据被丢弃）

## 2. BN（批量归一化）

在网络中加入BN（批量归一化）层。BN层中发生的事情是：对于输入BN层的任意mini-batch，假设该batch中有$N$个样本，每个样本维度为$D$。也即输入数据集$X$的规模为$N\times D$。我们先来看看一般的归一化方法：对于每个维度（每个特征）上的$X$都求均值和方差，并据此对每个维度上的数据进行归一化。对第$k$个维度，归一化之后的数据为：

$$\hat x_k=\frac{x_k-\mathrm{E}[x_k]}{\sqrt{\mathrm{Var}[x_k]}}$$

其中$\mathrm{E}[x_k]$是第$k$维所有$x$的均值，$\mathrm{Var}[x_k]$是第$k$维所有$x$的方差。这一过程通常发生在全连接层或卷积层之后，激活层之前。这么做虽然可以达成归一化的目的，但是一定程度上破坏了之前学习到的数据分布，因此采用以下方法改进：

对于batch中每个维度的数据，引入参数$\gamma_k,\beta_k$。并令它们的初始值为：
$$\gamma_k=\sqrt{\mathrm{Var}[x_k]},\ \beta_k=\mathrm{E}[x_k]$$

改进版的归一化中，我们使用$y_k=\gamma_k\hat{x}_k+\beta_k$来进行归一化操作。其中$\hat{x}_k$是刚才普通归一化得到的结果。之前提到，$\hat{x}_k$可能会导致数据分布损失，但是经过$y_k=\gamma_k\hat{x}_k+\beta_k$，$y_k$的值可以把归一化后的数据还原到原始数据，这样就保证开始训练时，之前的学习成果不会因为归一化而丢失。

与此同时，随着训练的进行，$\gamma_k,\beta_k$的值也会进行根据梯度下降进行调整，这样就可以使得数据从原始数据开始，逐渐归一化为符合高斯分布的数据，并将数据通过缩放和平移，变换到一个便于计算的区域，这样就既保存了之前的学习成果，又方便了之后的处理。妙蛙！(๑•̀ㅂ•́)و✧

## 3. 生成模型

### Pixel RNN/CNN

