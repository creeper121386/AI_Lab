# 种一棵决策树!
今天,你植树了吗?
***
![一棵决策树](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1524672521034&di=816e0cf3992a6f3d5ab1438a9feb67ed&imgtype=0&src=http%3A%2F%2Fimg.aichengxu.com%2F1%2F21%2F21353.png)


## 算法思想

决策树(decision)是一种常用的机器学习算法.既然叫做决策[树],自然是通过树结构来进行决策的.这种树结构很类似我们思考时,通过不断提出问题来进行判断.例如判断电影的好坏,我们常常会这么想:这部电影的音乐好不好?如果回答是"好",我们又会考虑"画面怎么样?",如果回答是"好",再看"演员演技如何?"如果回答仍然是"好",我们最终判断"这是一部好电影".

决策树的思想与之类似.用来训练的数据包含多个特征,而树结构包含一个根节点和若干叶节点和内部节点.在每个节点处,决策树都会根据样本的某个特征作出一次判断(正类似于"电影的画面怎么样"之类的判断),通过不断地作出判断,最终得到预测结果.

从功能上来看,决策树可以分为:用于预测连续值(完成回归任务)的回归树(如CART树),以及用于预测离散值(完成分类任务)的分类树(如ID3,C4.5).此处以ID3算法为例,用python进行决策树的构建.

另外,由于采用了树结构,决策树也使用到了分治的思想.

## 准备:数据导入及处理

此处使用的数据集是关于汽车购买的数据:通过汽车的各项特征来预测消费者对汽车的评价.使用numpy库中的readtxt函数进行导入,将测试集导入到一个array中.
### 划分验证集
除了测试集和训练集之外,这里还要用到验证集,用于验证模型的有效性.此处采用k折交叉验证法来进行验证集的划分:首先将待划分的数据集平均分成k个子集(k一般取10),然后循环进行k次训练,每次分别将k个子集中的一个当作验证集,其余的当作测试集.

另外,有些时候也会使用留一交叉验证法:即每次只把一个样本当作验证集.这样可以充分利用每个样本,但是会造成训练次数增多,因此常常在样本数量较少时使用.这里数据集数目较多,因此不采取这种方法.


## 树结构实现(模型训练)

接下来,我们要构建决策树的主体部分了.决策树的构建是通过递归来完成的.从树的根节点开始,使用训练集,递归的进行树的分支(划分).通过不断划分,训练集的数据也会从根节点不断分支,最终分布到叶节点上.当达到叶节点时,即停止划分.达到叶节点的情况有三种:
* 当前节点的数据类别都属于同一类X,无需再进行划分,此时将当前节点的类别标记为类别X;
* 没有任何一个数据符合当前节点的条件(即当前节点上数据为空),无法继续划分,此时将当前节点的类别标记为该节点处数据中出现最多的类别;
* 当前用来划分的依据(样本特征值)已经用尽,无法继续划分,此时将当前节点的类别标记为其父节点的类别.



## 处理: 预剪枝与后剪枝

## 数据测试

## 模型评估: 树模型绘制ROC曲线

### 一般情况下的ROC曲线

### 决策树的ROC曲线

对一个二分类的决策树,假设树有n个叶节点.训练数据时,每个叶节点处的数据都会有2种可能分布(正例或反例).因此整棵树的叶节点处数据分布的可能性有N=2^n种,这N种分布也就对应了N种不同的决策树分类器.

决策树的ROC空间中的N个点就由这N种不同的分类器(决策树)产生.对每种分类器(决策树),都可以计算出它对应的真阳性率和伪阳性率,得到在ROC空间上的一个点,N个点中最外围的点构成了决策树的ROC曲线.

### 优化ROC

这样得到的ROC空间中有2^n个点,计算量较大,因此对其进行优化:
