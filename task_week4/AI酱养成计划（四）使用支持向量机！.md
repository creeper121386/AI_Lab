# #4 支持向量机（SVM）
——有人觉得这是现成的最好的分类器
***
![SVM示意图](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1527170673&di=1dcc82cd28132c6c53838482d49ce094&imgtype=jpg&er=1&src=http%3A%2F%2Fs1.sinaimg.cn%2Fmw690%2F002xlA0Pgy6TFXnKcVi70%26amp%3B690)
## 线性可分问题
要了解SVM的概念，让我们先从**线性可分**的问题谈起。什么是线性可分问题呢？举以二分类问题为例，当一个分类器对数据集$\Bbb D=(x_1,y_1),(x_2,y_2),...(x_n,y_n)$进行分类时，假设其中任意样本$(x_i,y_i)$的特征向量$x_i$都是$m$维向量，那么数据集$\Bbb D$的特征就可以表示为$m$维向量空间中的一组点集。进行分类的过程其实就是找到一个**超平面**，可以把表示两种不同类别数据的点划分成相应的两部分$\{C_{+1},C_{-1}\}$。假如我们找到的超平面是线性的（例如二维空间的直线，三维空间的平面），那么这个分类问题就是线性可分问题
![](https://raw.githubusercontent.com/creeper121386/blog-file/master/%E6%B7%B1%E5%BA%A6%E6%88%AA%E5%9B%BE_%E9%80%89%E6%8B%A9%E5%8C%BA%E5%9F%9F_20180519150228.png)

事实上，对一个线性可分的数据集而言，这样的线性超平面不止一个。以上图中的数据为例，即使图中的直线略微左右倾斜，仍然可以正确地划分数据集。那么，在这些不同的直线中，我们需要找到一条最优的直线$L^*$。

所谓“最优”，指的是如果我我们向数据集中增添新的数据，直线仍然能很好的划分出两个类别，这就要求$L^*$正好处在两个类别的数据的“正中间”，换句话说，就是要求$\{C_{+1},C_{-1}\}$中靠近边界的点（这些点称作**支持向量点**）的距离$L^*$最近，也即求解最大间隔超平面。这就将求解$L^*$的问题转化为一个极值问题。

## 求解最大分隔超平面

### 函数间隔与几何间隔

假设我们要求的超平面为$L:f(x)=w^Tx+b$，其中$b$是截距，$w$是参数向量，且$w$的方向是超平面的法矢量方向，$x$表示点坐标（也即是特征值向量）。那么$\Bbb D$中的任一点$x_i$到$L$的距离为:
$$d_i=\frac{|w^Tx_i+b|}{||w||}$$

由于是二分类问题，我们使用$+1$和$-1$来标记样本的正反类。如果分类器能够正确分类的话，对样本$(x_i,y_i)$，有:
* 对于$y_i=+1$，有${w^Tx_i+b}>0$
* 对于$y_i=-1$，有${w^Tx_i+b}<0$

也就是说，无论$y$的取值如何，只要分类器能够正确分类，就有：
$$y_if(x_i)=y_i(w^Tx_i+b)\geqslant0 \qquad(1)$$

令$\gamma_i=y_if(x_i)$，$\gamma_i$称为点$x_i$到$L$的函数间隔。由于$|y|=1$，可以推出$\gamma_i=|w^Tx_i+b|$，进而得出：
$$d_i=\frac{\gamma_i}{||w||}$$

（这也是为什么要把$y$设定为$1$和$-1$，而不是像逻辑回归中一样设置为$0$和$1$的原因。）
这样就完成了求距离的工作，接下来考虑求解超平面$L$的最优解$L^*$。

### 拉格朗日算子法
还记得我们的目的吗？我们要求的是到支持向量点的距离最近的超平面$L^*$。首先我们要表示出支持向量到$L$的距离。假设支持向量到$L$的函数间隔为$\gamma_v$，那么要求解的极值问题就是：
$$\max_{w,b} \frac{\gamma_v}{||w||},\qquad(2.1)$$

$$ s.t.\;\ y_i(w^Tx_i+b)\geqslant\gamma_v,\; (x_i,y_i)\in\Bbb D \qquad(3.1)$$

其中式$(2)$是目标函数，式$(3)$是限制条件。式$(2)$的由来是：由于$\gamma_v$是支持向量到$L$的函数间隔，应当是所有点到$L$的函数间隔中最小的，因此任一点$x_i$到$L$的函数间隔$y_i(w^Tx_i+b)\geqslant\gamma_v$。由于$\gamma_v\geqslant0$，因此同时也保证了条件$(1)$，也即保证了我们求得的$L^*$是一个正确的分类器。

接下来对该极值问题进行简化：
* $w$是最终要求的变量，求$\max \limits_{w,b}\frac{\gamma_v}{||w||}$等价于求$\min \limits_{w,b}\frac{1}{2}{\gamma_v}||w||^2$
* 由于$\gamma_v$的取值并不影响最终求到的$L^*$，因此令$\gamma_v=1$。

得到简化后的极值问题：
$$\min_{w,b}\frac{1}{2}||w||^2,\qquad(2.2)$$

$$ s.t.\;\ y_i(w^Tx_i+b)\geqslant1,\; (x_i,y_i)\in\Bbb D \qquad(3.2)$$

这是一个凸二次优化问题。为了求解这类极值问题，我们可以使用**拉格朗日算子法**。构造拉格朗日函数：
$$\mathcal L(w,b,\alpha)=\frac12||w||^2+\sum_{i=1}^n \alpha_i(1-y_i(w^Tx_i+b))$$

其中$[\alpha_1,\alpha_2,...\alpha_n]$是拉格朗日算子。注意到式$(3.2)$是不等式约束关系，在拉格朗日算子法的使用中，假设出现不等式约束关系：
$$g_k(x)\leqslant0,\;(k=1,2,...n)$$

此时要求必须要满足**KKT条件(Karush-Kuhn-Tucker)**。在这里，对应的KKT条件是：
$$
\begin{cases}
\alpha_i\geqslant0,\\[2ex]
y_if(x_i)-1\geqslant0,\qquad\qquad(4.1)\\[2ex]
\alpha_i(y_if(x_i)-1)=0.
\end{cases}
$$

可以看到，对于不同取值的$\alpha_i$，要满足的KKT条件也不同。对于不同取值的$\alpha_i$，有：
$$
\begin{cases}
y_if(x_i)\geqslant1,\qquad\text{if }\;\alpha_i=0  \\
y_if(x_i)=1,\qquad\text{if }\;\alpha_i\gt0 
\end{cases}
\qquad(4.2)
$$

可以看到，当$\alpha_i>0$时，必有$\gamma_i=1$，也即对应的$x_i$必然是支持向量。事实上，$L^*$的取值也仅与支持向量有关，其他数据的分布并不影响超平面的选取（这也是SVM的好处之一）。到这里我们初步得到了求解$L^*$所需要的条件：包括式$(2.2),(3.2),(4.2)$。

### 软间隔与松弛变量


## 核函数
## 支持向量回归


    