# #1 种一棵决策树!
大家一起愉快地种树吧！OvO
***
![一棵决策树](https://timgsa.baidu。com/timg?image&quality=80&size=b9999_10000&sec=1524672521034&di=816e0cf3992a6f3d5ab1438a9feb67ed&imgtype=0&src=http%3A%2F%2Fimg.aichengxu.com%2F1%2F21%2F21353.png)


## 算法思想

**决策树(decision tree)** 是一种常用的机器学习算法。既然叫做决策**树**，自然是通过树结构来完成决策的。这种树结构很类似人类们思考时，通过不断提出问题来对事物进行判断。例如判断电影的好坏，我们常常会这么想:这部电影的音乐好不好?如果回答是"好"，我们又会考虑"画面怎么样?"，如果回答是"优秀"，再看"演员演技如何?"如果回答仍然是"精湛"，我们最终判断"这是一部好电影w"。



决策树的思想与之类似。我们用来训练的数据包含多个特征，而树结构包含一个**根节点**，若干**叶节点**和**内部节点**。在每个节点处，决策树都会根据样本的某个特征作出一次判断(正类似于"电影的画面怎么样"之类的判断)，通过不断地作出判断，最终得到预测结果。从功能上来看，决策树可以分为:
* 用于预测连续值(完成回归任务)的回归树(如CART树)，
* 用于预测离散值(完成分类任务)的分类树(如ID3，C4。5)。

此处以ID3算法为例，用python进行决策树的构建。另外，由于采用了树结构，决策树也使用到了分治的思想。

## 准备:数据导入及处理

此处使用的数据集是关于汽车购买的数据：通过汽车的各项特征来预测消费者对汽车的评价。使用numpy库中的`np.readtxt()`函数进行导入，将测试集导入到一个array中。整个程序中我们要用到的python库有numpy以及matplotlib。

### 划分验证集
除了测试集和训练集之外，这里使用到验证集，用于验证模型的有效性。此处采用**k折交叉验证法**来进行验证集的划分：首先将待划分的数据集平均分成k个子集(k一般取10)，然后循环进行k次训练，每次训练时分别将k个子集中的一个当作验证集，其余的当作测试集。

数据的导入和处理函数如下所示：
```python
def load(f_name， divide， delimiter):
    data = np。loadtxt("/media/why/DATA/why的程序测试/AI_Lab/Task/Task_week2/tree/" +
                      f_name， dtype=np。str， delimiter=delimiter)
    data = data[1:， :]
    if divide:
        label = data[:， -1:]
        data = data[:， :-1]
        return data， label
    else:
        return data
```

另外，有时也会用到留一交叉验证法:即每次只把一个样本当作验证集。这样可以充分利用每个样本，但是会造成训练次数增多，因此常常在样本数量较少时使用。这里数据集数目较多，所以不采取这种方法。


## 树结构实现(模型训练)

### **递归生成树结构**
接下来，我们要构建决策树的主体部分了。决策树的构建是通过递归来完成的：从树的根节点开始，使用训练集，递归进行树的分支(划分)。通过不断划分，训练集的数据也会从根节点开始被不断分类，最终分布到各个叶节点上。当达到叶节点时停止划分。达到叶节点的情况有三种:

* 当前节点的数据类别都属于同一类$X$，无需再进行划分，此时将当前节点的类别标记为类别$X$;
* 没有任何一个数据符合当前节点的条件(即当前节点上数据为空)，无法继续划分，此时将当前节点的类别标记为该节点处数据中出现最多的类别;
* 当前用来划分的依据(样本特征值)已经用尽，无法继续划分，此时将当前节点的类别标记为其父节点的类别。

当一个节点满足上述条件之一时，我们将它标记为叶节点，并返回该节点下的数据类别。由于使用递归的方法，叶节点的返回值会被其父节点接收，父节点接收返回值后，将该叶节点和其返回值加入到当前的子树中，不断重复这个过程，最终在根节点处返回一棵完整的决策树。


值得注意的是，在pyhon中无法使用指针来构造树形结构，而且每个节点的分叉数目事先无法确定，因此这里我们使用嵌套的 **字典（dict）** 来实现树结构。字典从外到内层层嵌套，分别代表从根节点到叶节点的逐层分叉。这样，在测试数据时，只要递归地从外到内遍历字典，就可以实现对树的遍历。

使用python进行递归构造的代码如下所示：
```python
def plant(data， label， feat_label， valiData， valiLabel):
    values， counts = np。unique(label， return_counts=True)
    if np。shape(values)[0] == 1:
        global leafNo
        leafNo += 1
        return (leafNo， label[0][0])
    if np。shape(data[0])[0] == 1:
        leafNo += 1
        return (leafNo， values[np。argmax(counts)])
    uqFeat = optimize(data， label， valiData， valiLabel)
    if type(uqFeat)。__name__ != 'int':
        leafNo += 1
        return (leafNo， uqFeat)
```

另外，在标记叶节点时，除了标记叶节点下的数据标签以外，同时为每个叶节点分配一个编号，之后画ROC曲线的时候会用到。

### **节点划分**
构造树的过程，实际上就是使用训练集进行训练的过程。在节点处，训练集数据被不断划分。那么接下来的问题是：在某一个节点处，使用样本的哪一个特征作为划分的依据呢？这就需要引入信息熵的概念。

为了对当前结点上的数据进行划分，我们一般希望划分之后，每个子结点上的数据类型尽可能相同，即每个子结点上的数据「纯度」要尽可能高，信息熵可以认为是一种描述信息纯度的指标。假设数据集$\Bbb D$包含n种不同的标签，第k种标签出现的概率为$p_k$，那么数据集$\Bbb D$的信息熵表示为：
$$Ent(\Bbb D)=-\sum_{k=1}^n  p_klog_2p_k$$

计算数据集信息熵的代码如下所示：
```python
def cal_ent(label):
    num = len(label)
    _， counts = np。unique(label， return_counts=True)
    prob = counts/num
    ent = (np。sum(prob*np。log2(prob)))
    return -ent
```
因此，假设某一节点处根据特征$a$来划分数据集$\mathbb D$，划分后的数据集为$ \{D^1,D^2,...D^m\} $，可以通过计算该节点处划分前后的信息熵，来得出根据特征$a$划分数据的信息增益：

$$ Gain(\Bbb D,a)=Ent(\mathbb D)-\sum_{k=1}^m \frac{|D^k|}{|\mathbb D|}Ent(D^k) $$


信息增益可以用来表示划分以后信息是否变得“更纯”，以及“纯度”增加了多少。因此，可以以此为依据，找出使得信息增益最大的划分方式。通过计算信息增益来优化划分方式的代码如下所示：
```python
def optimize(data, label, valiData, valiLabel):
    num = len(label)
    length = len(data[0])
    originEnt = cal_ent(label)
    maxGain = 0.0
    uqFeat = 0  # <-作为最佳划分依据的特征
    for i in range(length):
        _, new_label, _ = divide(data, label, i)
        sigma = 0
        for x in new_label:
            sigma += (cal_ent(x))*len(x)/num
        gain = originEnt-sigma
        if gain > maxGain:
            maxGain = gain
            uqFeat = i
    return uqFeat
```

### **开始训练**
封装好构造树函数以及优化函数之后，将训练集传入树函数，即可进行数据的训练，返回值是用来代表树结构的嵌套字典。到这里，我们就初步得到了一棵决策树。

## 数据测试

得到决策树之后，需要使用验证集来验证模型的有效性。在测试数据时，对于每一个测试样本，都要从根节点开始，沿着树枝一直走到叶节点，最后到达的叶节点就是该样本的预测结果。在此过程中，仍然使用递归的方法：

从根节点开始，将样本向下传递。对于当前结点:
* 如果当前节点下的子节点已经是叶节点，那么返回叶节点上的标签值作为预测结果；
* 如果当前节点下仍有子树，那么递归调用函数自身，将测试样本传递给子树。

对一个样本进行测试的代码如下所示：
```python
def classify(data, tree):
    for x in tree.keys():
        ix = x
    dict = tree[ix]
    for key in dict.keys():
        if data[ix] == key:
            if type(dict[key]).__name__ == 'dict':
                label = classify(data, dict[key])
            else:
                label = dict[key]
            return label
```


## 处理: 预剪枝与后剪枝
在构造决策树过程中，树的结构是由训练集决定的。决策树力图对训练集的数据分布尽可能地进行模拟，因此往往会出现过拟合的问题：决策树将样本学习得太好了，以至于对未曾见过的数据的泛化能力较差。为了减少过拟合问题，需要对决策树进行剪枝操作。

### **预剪枝**
预剪枝是在构建树结构时，在划分节点之前进行的。对节点进行划分前，先假设不划分该节点，即将该节点标记为叶节点，并比较一下划分前后，在该节点上使用训练集测试的正确率哪个更大。如果不划分时正确率更高，那么直接将该节点标记为叶节点，不对其进行展开，这样就实现了预剪枝工作。

要进行预剪枝工作，需要对之前的优化函数`optimize`进行改动，在`optimize`函数后添加代码：
```python
values, counts = np.unique(label, return_counts=True)
    maxValue = values[np.argmax(counts)]
    values, counts = np.unique(valiLabel, return_counts=True)
    acc1 = counts[np.where(values == maxValue)[0]]/len(valiLabel)

    _, new_label, featValue = divide(data, label, uqFeat)
    n = len(featValue)
    tmpLabel = []
    for i in range(n):
        values, counts = np.unique(new_label[i], return_counts=True)
        value = values[np.argmax(counts)]
        tmpLabel.append(value)
    count = 0
    for i in range(len(valiLabel)):
        for j in range(n):
            if featValue[j] == valiData[i][uqFeat] and tmpLabel[j] == valiLabel[i]:
                count += 1
    acc2 = count/len(valiLabel)
    if acc1 <= acc2:
        return uqFeat
    else:
        return maxValue
```

要注意，对于我们使用的汽车购买的数据集，由于数据集标签中‘unacc’标签占了绝大多数，因此在预剪枝时，内部节点被标记为叶节点'unacc'之后，得到的正确率甚至比划分节点之后的正确率还要高，因此决策树的大部分树枝都被剪掉了——结果导致决策树变成了决策树桩，这显然不是我们想要的，因此，可以根据数据集的实际情况，来决定是否进行预剪枝。

### **后剪枝**
后剪枝是在决策树构造完毕后，再对其进行剪枝。首先将用来剪枝的数据集传给决策树，使得数据集沿着数值被不断分类，最终分布到各个叶节点上。对于所有的叶节点，我们考察是否要进行剪枝——剪枝的含义是，将该叶节点的父节点标记为叶节点，并将父节点下的所有数据中，出现次数最多的标签作为父节点的标签。

判断是否进行剪枝操作的标准依然是数据集的正确率。对于该父节点下的样本，如果进行剪枝后正确率比剪枝前的更高，那么就进行剪枝操作。
* 剪枝前的正确率计算：将该父节点及其分支作为一棵子树，将该父节点下的所有数据分别作为测试数据，传递给`classify`函数，函数返回预测值，将预测值和样本的实际标签比较并计算出正确率。
* 剪枝后的正确率计算：假设该父节点下的所有数据为集合$\mathbb D$，$\mathbb D$中出现次数最多的标签集合为$L$，那么正确率为：
$$Acc=\frac {|L|}{|\mathbb D|}$$

后剪枝也使用了递归的方法：将用于剪枝的数据集`data`和其对应的标签集`label`传给函数，递归遍历整棵树，当到达叶节点时，对其父节点计算剪枝前后的样本正确率，并判断是否进行剪枝操作。若不剪枝，则返回原有的树结构，并继续寻找下一个叶节点;若选择剪枝，则返回剪枝后的叶节点。直到遍历过所有的叶节点为止，返回剪枝后的整棵决策树。

进行后剪枝的代码如下所示：
```python
def cut(tree, data, label):
    global newLeafNo
    if len(label) == 0:
        return tree
    for x in tree.keys():
        ix = x
    dict = tree[ix]
    for key in dict.keys():
        tmpData = []
        tmpLabel = []
        for i in range(len(data)):
            if data[i][ix] == key:
                tmpData.append(data[i])
                tmpLabel.append(label[i])
        tmpData = np.array(tmpData)
        tmpLabel = np.array(tmpLabel)
        if type(dict[key]).__name__ == 'dict':
            new_tree = cut(dict[key], tmpData, tmpLabel)
            tree[ix][key] = new_tree
        else:
            value, count = np.unique(label, return_counts=True)
            tmp = np.argmax(count)
            newAcc = count[tmp]/len(label)
            oldAcc = test(data, tree, label)
            if newAcc > oldAcc:
                tree = (newLeafNo, value[tmp])
                newLeafNo -= 1
                return tree
    return tree
```


## 模型评估: 树模型绘制ROC曲线

### 一般情况下的ROC曲线
ROC曲线常常用来评估二分类模型的性能。对于一个二分类任务，标签只有正例和反例存在（分别用1和0表示）。对于一个样本，实际的类别和预测结果有以下四种情况：

||实际类别为1|实际类别为0|
|:--:|:--:|:--:|:--:|
|预测类别为1|真正例(记作TP)|假正例(记作FP)|
|预测类别为0|假反例(记作FN)|真反例(记作TN)|

规定**真正例率(TPR)** 和**假正例率(FPR)** 分别为：
$$ TPR=\frac{TP}{TP+FN},FPR=\frac{FP}{FP+TN}$$
ROC曲线就是由TPR和FPR分别作为x轴和y轴作出的图像。对于一个测试集，只计算出一组对应的TPR和FPR值，此时的ROC曲线只包含一个点。为了得到一条曲线，我们需要多组TPR和FPR值。

在二分类任务中，模型最终给出的结果往往不是0或1的准确预测值，而是一个0～1的数值。为了得到0或1的结果，往往会人为规定一个阈值（通常是0.5），当预测结果大于这个值的时候，就认为结果是1，反之是0。同一个模型，当阈值不同时可以认为是不同的分类器。因此，我们可以通过改变阈值来得到多个分类器，以从一个测试集上得到多组TPR和FPR值，从而画出ROC曲线。

### 决策树的ROC曲线

而对一个二分类的决策树，ROC曲线的绘制就有些困难：决策树返回的是0或1的离散值，而不是0～1的连续值。因此无法通过改变阈值来得到多组TPR和FPR。

对此，论文 ***Learning Decision Trees Using the Area Under the ROC Curve*** 给出了一种方法：假设决策树有n个叶节点。训练数据时，每个叶节点处的数据都会有2种类别(正例或反例)。而我们为该叶节点标记的标签的也只有0和1两种可能。因此整棵树的叶节点的标记的可能性有$N=2^n$种，这N种不同的标记方法也就对应了$N$种不同的决策树分类器，决策树的ROC空间中的$N$个点就由这$N$种不同的分类器(决策树)产生。

对一每种分类器(决策树)，都可以计算出它对应的TPR和FPR，从而得到在ROC空间上的一个点，这样得到的$N$个点就构成了决策树的ROC曲线。

### 优化ROC

这样得到的ROC空间中，一共有$2^n$个点，计算量较大。事实上，这其中很多点都是不必要的，因此可以对其进行优化，优化之后的点只有$n+1$个，即只需要考虑叶节点上$2^n$中个分类器中的$n+1$种，大大减少了计算量。我们将这n+1种分类器记作$\{S_0,S_1,S_2...S_n\}$。

假设一棵决策树有3个叶节点(也即$n=3$)，并假设将数据集传给决策树后，每个叶节点上的数据分布如下所示：
||正例数|反例数|
|-|:-:|:-:|
|leaf1|3|5|
|leaf2|5|1|
|leaf3|4|2|
首先按照 **局部正精度(local positive accuracy)** 来对各个叶节点降序排列。（某一叶节点的局部正精度$=\frac {\text {当前叶节点正例数}}{\text {当前叶节点总样本数}}$）排列好之后，我们假定分类器S在每个叶节点上的标签为+或-（分别代表正例或反例），优化后的所有S在各个叶节点上的分布符合阶梯形，如下所示：
||正例数|反例数|$S_0$|$S_1$|$S_2$|$S_3$|
|-|:-:|:-:|:-:|:-:|:-:|:-:|
|leaf1|5|1|-|+|+|+|
|leaf2|4|2|-|-|+|+|
|leaf3|3|5|-|-|-|+|
于是，我们最终得到的优化后的分类器为：
$$S_0=\{-,-,-\},S_1=\{+,-,-\},S_2=\{+,+,-\},S_3=\{+,+,+\}$$
根据这n+1个分类器，就可以得到ROC曲线上的点，从而作出ROC曲线。绘制ROC曲线的代码如下所示(在此省去计算ROC曲线上点的过程)：
```python
def draw_ROC(data, test_label, tree, leafNum):
    values = np.unique(test_label)
    num = len(test_label)
    k = 0
    for x in values:
        label = []
        pred_label = pred(data, tree)
        pred_leaf = []
        for i in range(num):
            label.append(1 if test_label[i] == x else 0)
            pred_leaf.append(pred_label[i][0])
        uqLeaf = np.unique(pred_leaf)
        posAcc = []
        for l in uqLeaf:
            index = np.where(pred_leaf == l)[0]
            posNum = 0
            for ix in index:
                if label[ix] == 1:
                    posNum += 1
            posAcc.append([l, posNum/len(index)])
        maxLeaf = sorted(posAcc, key=lambda temp: temp[1], reverse=True)
        maxLeaf = [y[0] for y in maxLeaf]
        leaves = [0]*(leafNum+1)    # 按照排序后的叶节点顺序,表示该分类器下,对应所有叶节点的分类情况
        x_ROC = []
        y_ROC = []
        x_PR = []
        y_PR = []
        for i in range(leafNum+1):
            leaves[i] = 1
            newPredLabel = []
            for j in range(num):
                tmp = pred_leaf[j]
                ix = maxLeaf.index(tmp)
                newPredLabel.append(leaves[ix])
            Tp, Fp = cal_ROC(newPredLabel, label)
            p1, p2 = cal_PR(newPredLabel, label)
            x_ROC.append(Fp)
            y_ROC.append(Tp)
            x_PR.append(p1)
            y_PR.append(p2)
        fig = plt.figure(k)
        ax = fig.add_subplot(1, 1, 1)

        ax.set_xlabel('False Postive Rate')
        ax.set_ylabel('True Postive Rate')
        ax.set_title('ROC Curve of Decision Tree (pre-pruning)')
        plt.plot(x_ROC, y_ROC)
        plt.scatter(x_ROC, y_ROC, alpha=0.6)
        plt.show()
        k += 1
```

至此，我们就完成了ID3决策树的所有工作。大家来一起愉快地种树吧！
***
$\bf 2018.4.26$ $\bf by$ $\bf WHY$

