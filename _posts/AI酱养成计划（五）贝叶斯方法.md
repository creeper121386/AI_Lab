---
title: AI酱养成计划（五）贝叶斯方法
date: 2018-07-08 10:26:51
tags:
- 机器学习
- python
categories: 机器学习
---

# #5 贝叶斯方法&概率论模型

![概率论只不过是把常识用数学公式表达了出来。——拉普拉斯](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1531027306977&di=68a2665db7a9d7be828978cf286329eb&imgtype=0&src=http%3A%2F%2Fp0.ifengimg.com%2Fpmop%2F2018%2F0330%2FCDB922FBF5536ECAC4536C05BA6C87776ACBA145_size21_w468_h263.jpeg)

***

<!--more-->

$\mathbf{Attention:}$阅读本文需要一些概率论知识(。・・)ノ

## 贝叶斯理论

贝叶斯方法是一种基于概率论的重要的机器学习方法。概率论试图将复杂的事件和数据分布用数学的语言表示，对于机器学习要处理的数据而言，以分类问题为例，实际上就是求对于给定样本$X$，其标签为$y$的概率（后验概率）。只要得到标签集合$Y$中所有标签的概率，那么出现概率最大的$y$就是最终的分类结果，这样就完成了分类任务。

但是对于给定的$x$，一般很难求出其后验概率。因此这里使用到贝叶斯公式：

$$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$$

贝叶斯公式给出了后验概率和先验概率之间的关系，因此可以方便地根据$P(X|Y)$计算出后验概率。贝叶斯方法多是基于这个公式而来的。接下来我们以分类问题为例进行讨论。假设给定训练集$\Bbb D=\{(X_1,y_1),(X_2,y_2)...(X_n,y_n)\}$，其中$X_i=[x_1,x_2...x_m]$是特征向量，$y_i$是类别标签。

## 朴素贝叶斯

朴素贝叶斯是最简单的贝叶斯方法。其简单之处在于两个假设（独立同分布假设，记作$i.i.d$假设）：

* 假设$X$的各个特征$x$相互独立，互不影响
* 假设这些特征对于分类的贡献同等重要。

当然，事实上这两个条件都很难成立，但是对于某些问题，特征之间的独立性影响并不大，这样的假设可以大大简化模型，这也正是“朴素”一词的含义。因此朴素贝叶斯模型的训练过程，实际上就是统计和计算各个概率的过程。需要计算的有：

$$
\begin{cases}
P(X|y)=\prod_{i=1}^n P(x_i|y) \\
P(X)=\prod_{i=1}^n P(x_i)
\end{cases}
$$

$P(y)$则直接根据统计得出。要注意，上述计算方法仅适用于特征分布是**离散值**的情况下（例如人的性别只有男女两种可能取值）。如果特征属性是连续值的情况下（例如人的身高），在计算上述概率时，假设数据都遵从**高斯分布**，使用极大似然法估计出概率密度函数来进行计算。

## 贝叶斯网络

### 概率图模型

在现实中，上述的两个假设很多时候是无法成立的。即样本的各个特征之间并非相互独立，而是有着因果关系（依赖关系）。这个时候独立同分布假设就失去了作用，我们需要想办法表示出这些不同属性之间的依赖关系，这种办法就是贝叶斯网络。

贝叶斯网络使用有向无环图来表示这些依赖关系：

* 使用图的节点来表示各个特征（相当于概率论中的随机变量）。
* 使用有向边表示特征之间的相互依赖关系。例如$A\to B$表示$B$依赖$A$。
* 在各节点处计算出当前节点（假设为$\alpha$）与其所有父节点（假设为$\pi$）的联合概率，即$P(\alpha|\pi)$。

要说明的是，贝叶斯网将样本的标签也当做了一种特征，作为网络的一个普通节点。事实上，贝叶斯网不仅可以通过已知的$X$来对标签$y$进行推断，也可以根据样本的各个类别和标签中任意个已知量来推断其他未知量的值。假设一个贝叶斯网络的**结构已经确定**，那么其训练过程就是计算各个依赖关系对应的联合概率的过程。

### 利用贝叶斯网进行推断

假设$(X,y)$的$m$个特征值和类别标签$y$这$m+1$个变量中，已经观测到的变量集合的取值为$E=e$（称为“证据变量”），其余待推断的变量集合为$H$，那么我们要推断的$H$的取值$h$应该是使得后验概率$P(H=\mathbf h|E=\mathbf e)$最大的取值$h$。即：

$$h=arg\max_h P(H=\mathbf h|E=\mathbf e)$$

这样一来，就需要计算$H$的各个可能取值下的后验概率$P(H=\mathbf h|E=\mathbf e)$。当未知变量只有类别标签$y$时，可以根据$P(y|X)=\frac{P(X,y)}{P(X)}$直接计算后验概率。要注意这里联合概率的计算要考虑变量之间的依赖关系，计算方法为：

$$P(x_1,x_2...x_n)=\prod_{i=1}^mP(x_i|\pi_i)$$

其中$\pi_i$表示$x_i$节点的父节点集合。这样根据这个式子就可以容易地计算出：

$$P(X|y)=\frac{P(y|\pi_y)\prod_{i=1}^mP(x_i|\pi_i)}{\sum_y\prod_{i=1}^mP(x_i|\pi_i)}\qquad \quad(1)$$

其中分母表示在$y$取不同值时，分别对应的$X$的概率之和。

### 马尔科夫链/蒙特卡罗方法（MCMC）&吉布斯采样

然而不幸的是，在对多个未知变量进行推断时，直接计算后验概率是NP难的，复杂度为$O(2^n)$，因此在实际的推断中我们常常采用**模糊推断**的办法。

模糊推断是靠马尔科夫链/蒙特卡罗方法来完成的。这里使用一种简单的蒙特卡洛方法：吉布斯采样。我们的目标是在已经观测到证据变量$E=\{e_1,e_2...e_p\}$的情况下，求出未知变量$H=\{h_1,h_2...h_q\}$的任意一个取值$\mathbf h'$出现的概率，即$P(\mathbf h'|E=e)$吉布斯采样的大致流程如下：

* 初始化计数器$i=0$，并事先规定采样轮数$T$
* 假设证据变量的取值为$\mathbf e$，那么首先随机产生$H$的取值，假设为$\mathbf h_0$
* 对$H$中的每个元素$H_i$都进行采样，具体做法是：计算出针对$H_i$的后验分布$P(H_i|\text{除}H_i\text{之外的所有变量})=P(H_i|\mathbf e,h_1,h_2...h_{i-1},h_{i+1},...h_q)$，计算方法与式$(1)$相同，然后将$H_i$的取值$h_i$更新为出现概率最大的$h_i$。即
 
    $$H_i=arg\max_{H_i}P(H_i|\mathbf e,h_1,h_2...h_{i-1},h_{i+1},...h_q)\qquad(2)$$

* 将未知变量集合$H$整个更新过一遍后，如果此刻恰好$H=\mathbf h'$，为$i$增加一个计数$(i+=1)$
* 经过全部$T$轮采样后，要求的$P(\mathbf h'|E=e)\approx \frac{i}{T}$。

注意到，MCMC方法其实是在未知特征变量张成的样本空间中进行**随机漫步**，并且每一步只与前一步的状态有关，这正是一个马尔科夫链。当$t \to \infty$时，马尔科夫链将会收敛于一个稳定的分布，因此可以使用这样的方法来逼近$P(\mathbf h'|E=e)$。

使用MCMC得出$H$的后验分布之后，只要根据式$(2)$，从其中选取概率最大的分布状态，就可以作为贝叶斯网的推断结果。

### 期望优化（EM）算法

现在让我们更进一步——在实际的训练中，由于种种原因，训练集经常会有一些变量信息缺失（比如某些特征无法观测）。对于依赖统计的贝叶斯网而言，模型参数$\Theta$就是各个节点处的概率分布信息。这么一来，数据的缺失就意味着参数的缺失，导致网络的残缺。为了应对缺失的变量，贝叶斯网络采用的方法是**EM算法**。假设可观测变量集合为$X$，无法观测的变量集合为$Z$，EM算法的大致流程如下：

* 随机初始化参数的未知部分，得到初始参数集合$\Theta_0$
* E步（期望）：根据参数$\Theta_0$，推断出缺失数据的期望$Z$。具体做法是，对于某个节点$x$处的变量取值，求出其概率分布，并求加权和。即$E(x)=\sum_x P(x)x$
* M步（最大化）：根据推断出的新的数据集，寻找最大化似然的参数。具体做法是直接统计并计算出参数，作为新的$\Theta_1$
* 重复E步和M步，直到收敛，就可以得到最终填充的数据，以及最终确定的网络参数$\Theta$

可以看出，EM算法类似坐标下降法，每次只优化参数和样本数据这二者中的一个，通过不断迭代来达到收敛，这样就解决了部分数据缺失的问题。

### 结构学习

接下来让我们再进一步——之前讨论的都是假设**网络结构已经确定**的情况下，网络的运行方式。但是在实际的训练中，网络的结构往往是最难确定的。在变量个数较少时，可以由领域专家来构造网络结构，但是当数据量很大时，网络结构有多种可能性，不同的网络结构可能对结果造成很与大影响，这个时候就要想办法进行网络结构的学习。

选择网络结构的一般方法是：在已知数据集$\Bbb D$的情况下，首先确定一个评分函数，然后在网络的各种可能结构中，选择出使评分函数最大（或最小）的一个结构。我们的选择标准（评分函数）不同，最终得到的网络结构也不同。假设贝叶斯网表示为$B=(G,\Theta)$，其中$G$表示网络结构，$\Theta$表示网络参数。常用的评分函数如下：

* 贝叶斯评分：贝叶斯网结构的后验概率的的对数似然函数。也即：

    $$logP(\Bbb D|G)+logP(G)$$
    
    其中第一项是网络结构的对数似然，第二项是结构先验分布（一般假设是均匀分布）。其中$P(\Bbb D|G)$可以展开为:
    
    $$P(\Bbb D|G)=\int P(\Bbb D|G,\Theta)p(\Theta|G)d\Theta$$

    直观上来讲，贝叶斯评分选择的是能够最好地拟合训练数据的网络结构。
* 贝叶斯信息准则($BIC$)：使用拉普拉斯近似，并综合考虑使得网络的编码长度尽量短，可以得到评分准则：

    $$BIC(\Bbb D|G)=logP(\Bbb D|G,\Theta)-\frac{|B|}{2}logm$$

    （讲道理这里BIC的推导我也没懂....有明白的同学阔以在评论区留言！笔芯！）

### 结构优化

在确定评分函数之后，就可以对结构进行优化了。这是一个图搜索的问题，显然如果使用穷举的方法，计算将是$NP$难的。因此实际使用中我们常常使用启发式的方法。假设我们从一个已经确定的初始的网络结构$G_0$开始（初始的网络最终还是要人工确定...），通过不断迭代的优化方法来逼近评分函数的最值：

* 使用搜索算子（搜索算子包括加边、减边和转边）对当前模型进行局部修改，得到一系列候选模型
* 计算所有候选模型的评分函数，取最优的候选模型作为新的模型
* 重复上述两步，直到收敛。

## 马尔科夫模型

### 马尔科夫链

首先要介绍的是**马尔科夫链**。马尔科夫链指的是一系列随机变量$x_1,x_2...x_n$，任意一个随机变量$x_t$都只受前一个随机变量$x_{t-1}$的影响，$x_t$与除了$x_{t-1}$之外的其他变量都相互独立。事实上，这就是一个简单的贝叶斯网络。

$$\boxed{x_1}\to \boxed{x_2}\to \boxed{x_3}\to \boxed{x_4}......\to \boxed{x_n}$$

这个模型十分简单有效，在NLP领域有广泛的运用。例如对语料进行情感分析时，语句作为一个序列，恰好符合马尔科夫假设的特点——一般来说，一个句子中的词常常与它的前一个词相关。我们只要根据贝叶斯网络的训练及推断方法，就可以完成马尔科夫链的训练和推断。

### 隐变量

而隐马尔科夫模型，则是在马尔科夫链中引入了“隐变量”的概念。假设一个马尔科夫链中的随机变量$x_1,x_2,...x_n$无法直接观测到，但是每一个随机变量$x$都可以产生一个可以观测的信号$y$，隐马尔科夫模型就是通过观测$y$的值，来进行推断。

$$
\begin{matrix}
&\boxed{y_1}& \quad\boxed{y_2}& \quad\boxed{y_3}&\quad\boxed{y_4}&\quad\quad\;\;\boxed{y_n}& \\
& \uparrow &\quad \uparrow & \quad\uparrow &\quad\uparrow &\quad\quad\;\; \uparrow &\\
& \boxed{x_1}&\to \boxed{x_2}&\to \boxed{x_3}&\to \boxed{x_4}&......\quad \boxed{x_n} &
\end{matrix}
$$

### 马尔科夫随机场




***

（未完待续）

突然觉得贝叶斯好难啊啊啊
贝叶斯网引论273页，看完了再更
（溜走
