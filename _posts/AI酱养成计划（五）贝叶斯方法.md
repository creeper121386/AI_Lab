---
title: AI酱养成计划（五）贝叶斯方法
date: 2018-07-08 10:26:51
tags:
- 机器学习
---

# #5 贝叶斯方法

![概率论只不过是把常识用数学公式表达了出来。——拉普拉斯
](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1531027306977&di=68a2665db7a9d7be828978cf286329eb&imgtype=0&src=http%3A%2F%2Fp0.ifengimg.com%2Fpmop%2F2018%2F0330%2FCDB922FBF5536ECAC4536C05BA6C87776ACBA145_size21_w468_h263.jpeg)

***

$\bold{Waining:}$阅读本文需要一些概率论知识(。・・)ノ

## 贝叶斯理论

贝叶斯方法是一种基于概率论的重要的机器学习方法。概率论试图将复杂的事件和数据分布用数学的语言表示，对于机器学习要处理的数据而言，以分类问题为例，实际上就是求对于给定样本$X$，其标签为$y$的概率（后验概率）。只要得到标签集合$Y$中所有标签的概率，那么出现概率最大的$y$就是最终的分类结果，这样就完成了分类任务。

但是对于给定的$x$，一般很难求出其后验概率。因此这里使用到贝叶斯公式：

$$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$$

贝叶斯公式给出了后验概率和先验概率之间的关系，因此可以方便地根据$P(X|Y)$计算出后验概率。贝叶斯方法多是基于这个公式而来的。接下来我们以分类问题为例进行讨论。假设给定训练集$\Bbb D=\{(X_1,y_1),(X_2,y_2)...(X_n,y_n)\}$，其中$X_i=[x_1,x_2...x_m]$是特征向量，$y_i$是类别标签。

## 朴素贝叶斯

朴素贝叶斯是最简单的贝叶斯方法。其简单之处在于两个假设（独立同分布假设）：

* 假设$X$的各个特征$x$相互独立，互不影响
* 假设这些特征对于分类的贡献同等重要。

当然，事实上这两个条件都很难成立，但是对于某些问题，特征之间的独立性影响并不大，这样的假设可以大大简化模型，这也正是“朴素”一词的含义。因此朴素贝叶斯模型的训练过程，实际上就是统计和计算各个概率的过程。需要计算的有：

* $P(X|y)=\prod_{i=1}^n P(x_i|y)$
* $P(X)=\prod_{i=1}^n P(x_i)$
* $P(y)$直接根据统计得出。

要注意，上述计算方法仅适用于特征分布是**离散值**的情况下（例如人的性别只有男女两种可能取值）。如果特征属性是连续值的情况下（例如人的身高），在计算上述概率时，假设数据都遵从**高斯分布**，使用极大似然法估计出概率密度函数来进行计算。

## 贝叶斯网络

### 使用概率图模型

在现实中，上述的两个假设很多时候是无法成立的。即样本的各个特征之间并非相互独立，而是有着因果关系（依赖关系）。这个时候独立同分布假设就失去了作用，我们需要想办法表示出这些不同属性之间的依赖关系，这种办法就是贝叶斯网络。

贝叶斯网络使用有向无环图来表示这些依赖关系：

* 使用图的节点来表示各个特征（相当于概率论中的随机变量）。
* 使用有向边表示特征之间的相互依赖关系。例如$A\to B$表示$B$依赖$A$。
* 在各条边处计算出当前边连接的两个变量（特征）的联合概率。

要说明的是，贝叶斯网将样本的标签也当做了一种特征，作为网络的一个普通节点。事实上，贝叶斯网不仅可以通过已知的$X$来对标签$y$进行推断，也可以根据样本的各个类别和标签中任意个已知量来推断其他未知量的值。假设一个贝叶斯网络的**结构已经确定**，那么其训练过程就是计算各个依赖关系对应的联合概率的过程。

### 推断

假设$(X,y)$的$m$个特征值和类别标签$y$这$m+1$个变量中，已经观测到的变量集合的取值为$E=e$（称为“证据变量”），其余待推断的变量集合为$H$，那么我们要推断的$H$的取值$h$应该是使得后验概率$P(H=\bold h|E=\bold e)$最大的取值$h$。即：

$$h=arg\max_h P(H=\bold h|E=\bold e)$$

这样一来，就需要计算$H$的各个可能取值下的后验概率$P(H=\bold h|E=\bold e)$。然而不幸的是，这样的计算是NP难的，复杂度是$O(2^n)$，因此在实际的推断中我们常常采用**模糊推断**的办法.

### 吉布斯采样

模糊推断是靠吉布斯采样法来完成的。我们的目标是在已经观测到证据变量$E=\{e_1,e_2...e_p\}$的情况下，求出未知变量$H=\{h_1,h_2...h_q\}$的任意一个取值$\bold h'$出现的概率。吉布斯采样的大致流程如下：

* 初始化计数器$i=0$，并事先规定采样轮数$T$
* 假设证据变量的取值为$\bold e$，那么首先随机产生$H$的取值，假设为$\bold h_0$
* 对$H$中的每个元素$H_i$都进行采样，具体做法是：计算出针对$H_i$的后验分布$P(H_i|\text{除}H_i\text{之外的所有变量})=P(H_i|\bold e,h_1,h_2...h_{i-1},h_{i+1},...h_q)$，并将$H_i$的取值$h_i$更新为出现概率最大的$h_i$。即
 
    $$H_i=arg\max_{H_i}P(H_i|\bold e,h_1,h_2...h_{i-1},h_{i+1},...h_q)$$
* 将未知变量集合$H$整个更新过一遍后，如果此刻恰好$H=\bold h'$，为$i$增加一个计数$(i+=1)$
* 经过全部$T$轮采样后，要求的$P(\bold h'|E=e)\approx \frac{i}{T}$。